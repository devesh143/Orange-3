Content,sentiment
string,continuous
meta,
"Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is also the field of study in computer science that develops and studies intelligent machines. ""AI"" may also refer to the machines themselves.
AI technology is widely used throughout industry, government and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), and competing at the highest level in strategic games (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding, but after 2012, when deep learning surpassed all previous AI techniques, there was a vast increase in funding and interest.
The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.
To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and many other fields.


== Goals ==
The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.


=== Reasoning, problem-solving ===
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": they became exponentially slower as the problems grew larger.
Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.
Accurate and efficient reasoning is an unsolved problem.


=== Knowledge representation ===
Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas.A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as:
objects, properties, categories and relations between objects;

situations, events, states and time;
causes and effects;
knowledge about knowledge (what we know about what other people know);default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.
Among the most difficult problems in KR are: the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).Knowledge acquisition is the difficult problem of obtaining knowledge for AI applications. Modern AI gathers knowledge by ""scraping"" the internet (including Wikipedia). The knowledge itself was collected by the volunteers and professionals who published the information (who may or may not have agreed to provide their work to AI companies). This ""crowd sourced"" technique does not guarantee that the knowledge is correct or reliable. The knowledge of Large Language Models (such as ChatGPT) is highly unreliable -- it generates misinformation and falsehoods (known as ""hallucinations""). Providing accurate knowledge for these modern AI applications is an unsolved problem.


=== Planning and decision making ===
An ""agent"" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.
In automated planning, the agent has a specific goal. In automated decision making, the agent has preferences – there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision making agent assigns a number to each situation (called the ""utility"") that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.In classical planning, the agent knows exactly what the effect of any action will be.
In most real-world problems, however, the agent may not be certain about the situation they are in (it is ""unknown"" or ""unobservable"") and it may not know for certain what will happen after each possible action (it is not ""deterministic""). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.
In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning) or the agent can seek information to improve its preferences.Information value theory can be used to weigh the value of exploratory or experimental actions.
The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain what the outcome will be.
A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way, and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g. by iteration), be heuristic, or it can be learned.Game theory describes rational behavior of multiple interacting agents, and is used in AI programs that make decisions that involve other agents.


=== Learning ===
Machine learning is the study of programs that can improve their performance on a given task automatically.
It has been a part of AI from the beginning.There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).
In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as ""good"".Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning uses artificial neural networks for all of these types of learning.
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.


=== Natural language processing ===
Natural language processing (NLP) allows programs to read, write and communicate in human languages such as English.
Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation
unless restricted to small domains called ""micro-worlds"" (due to the common sense knowledge problem).
Modern deep learning techniques for NLP include word embedding (how often one word appears near another), transformers (which finds patterns in text), and others. In 2019, generative pre-trained transformer (or ""GPT"") language models began to generate coherent text, and by 2023 these models were able to get human-level scores on the bar exam, SAT, GRE, and many other real-world applications.


=== Perception ===
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.
The field includes speech recognition,image classification,facial recognition, object recognition,
and robotic perception.


=== Robotics ===
Robotics uses AI.


=== Social intelligence ===
Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.
For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.


=== General intelligence ===
A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.


== Tools ==
AI research uses a wide variety of tools to accomplish the goals above.


=== Search and optimization ===
AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.


==== State space search ====
State space search searches through a tree of possible states to try to find a goal state.
For example, Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.Simple exhaustive searches
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.
""Heuristics"" or ""rules of thumb"" can help to prioritize choices that are more likely to reach a goal.Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.


==== Local search ====
Local search uses mathematical optimization to find a numeric solution to a problem. It begins with some form of a guess and then refines the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. This process is called stochastic gradient descent.Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses).Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).Neural networks and statistical classifiers (discussed below), also use a form of local search, where the ""landscape"" to be searched is formed by learning.


=== Logic ===
Formal Logic is used for reasoning and knowledge representation.
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as ""and"", ""or"", ""not"" and ""implies"")
and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as ""Every X is a Y"" and ""There are some Xs that are Ys"").Logical inference (or deduction) is the process of proving a new statement (conclusion) from other statements that are already known to be true (the premises).
A logical knowledge base also handles queries and assertions as a special case of inference.
An inference rule describes what is a valid step in a proof. The most general inference rule is  resolution.
Inference can be reduced to performing a search to find a path that leads from premises to conclusions, where each step is the application of an inference rule.
Inference performed this way is intractable except for short proofs in restricted domains. No efficient, powerful and general method has been discovered.Fuzzy logic assigns a ""degree of truth"" between 0 and 1 and handles uncertainty and probabilistic situations.Non-monotonic logics are designed to handle default reasoning.
Other specialized versions of logic have been developed to describe many complex domains (see knowledge representation above).


=== Probabilistic methods for uncertain reasoning ===
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks
are a very general tool that can be used for many problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks)
and perception (using dynamic Bayesian networks).Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,
and information value theory.
These tools include models such as Markov decision processes,

dynamic decision networks,game theory and mechanism design.


=== Classifiers and statistical learning methods ===
The simplest AI applications can be divided into two types: classifiers (e.g. ""if shiny then diamond""), on one hand, and controllers (e.g. ""if diamond then pick up""), on the other hand. Classifiers
are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an ""observation"") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the ""most widely used learner"" at Google, due in part to its scalability.Neural networks are also used as classifiers.


=== Artificial neural networks ===
Artificial neural networks were inspired by the design of the human brain: a simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. In practice, the input ""neurons"" are a list of numbers, the ""weights"" are a matrix, the next layer is the dot product (i.e., several weighted sums) scaled by an increasing function, such as the logistic function. ""The resemblance to real neural cells and structures is superficial"", according to Russell and Norvig.  Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.
Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.In feedforward neural networks the signal passes in only one direction.Recurrent neural networks feed the output signal back into the input, which allows  short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.Perceptrons
use only a single layer of neurons, deep learning uses multiple layers.
Convolutional neural networks strengthen the connection between neurons that are ""close"" to each other – this is especially important in image processing, where a local set of neurons must identify an ""edge"" before the network can identify an object.


=== Deep learning ===
Deep learning
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification
and others. The reason that deep learning performs so well in so many applications is not known as of 2023.
The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)
but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.


=== Specialized hardware and software ===

In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow  software, had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.
Historically, specialized languages, such as Lisp, Prolog, and others, had been used.


== Applications ==

AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search),
targeting online advertisements,recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),autonomous vehicles (including drones,
ADAS and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and
image labeling (used by Facebook, Apple's iPhoto and TikTok).
There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported they had incorporated ""AI"" in some offerings or processes.
A few examples are energy storage,
medical diagnosis,
military logistics,
applications that predict the result of judicial decisions,foreign policy,
or supply chain management.
Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then it defeated Ke Jie in 2017, who at the time continuously held the world No. 1 ranking for two years. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.In the early 2020s, generative AI gained widespread prominence. ChatGPT, based on GPT-3, and other large language models, were tried by 14% of Americans adults. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.


== Ethics ==
AI, like any powerful technology, has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to ""solve intelligence, and then use that to solve everything else"". However, as the use of AI has become widespread, several unintended consequences and risks have been identified.


=== Risks and harm ===


==== Algorithmic bias and fairness ====

Machine learning applications will be biased if they learn from biased data.
The developers may not be aware that the bias exists.
Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.Fairness in machine learning is the study of how to prevent the harm caused by algorithmic bias. It has become serious area of academic study within AI. Researchers have discovered it is not always possible to define ""fairness"" in a way that satisfies all stakeholders.On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as ""gorillas"" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called ""sample size disparity"". Google ""fixed"" this problem by preventing the system from labelling anything as a  ""gorilla"". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.
In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different -- the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as ""race"" or ""gender""). The feature will correlate with other features (like ""address"", ""shopping history"" or ""first name""), and the program will make the same decisions based on these features as it would on ""race"" or ""gender"".
Moritz Hardt said “the most robust fact in this research area is that fairness through blindness doesn't work.”Criticism of COMPAS highlighted a deeper problem with the misuse of AI. Machine learning models are designed to make ""predictions"" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. Unfortunately, if an applications then uses these predictions as recommendations, some of these ""recommendations"" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is necessarily descriptive and not proscriptive.Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.


==== Lack of transparency ====

Most modern AI applications can not explain how they have reached a decision. The large amount of relationships between inputs and outputs in deep neural networks and resulting complexity makes it difficult for even an expert to explain how they produced their outputs, making them a black box.There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example,  Justin Ko and Roberto Novoa developed a system that could identify skin diseases better than medical professionals, however it classified any image with a ruler as ""cancerous"", because pictures of malignancies typically include a ruler to show the scale. A more dangerous example was discovered by Rich Caruana in 2015: a machine learning system that accurately predicted risk of death classified a patient that was over 65, asthma and difficulty breathing as ""low risk"". Further research showed that in high-risk cases like this, the hospital would allocate more resources and save the patient's life, decreasing the risk measured by the program. Mistakes like these become obvious when we know how the program has reached a decision. Without an explanation, these problems may not not be discovered until after they have caused harm.
A second issue is that people who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are required to clearly and completely explain the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.DARPA established the XAI (""Explainable Artificial Intelligence"") program in 2014 to try and solve these problems.There are several potential solutions to the transparency problem. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network have learned and produce output that can suggest what the network is learning. Supersparse linear integer models use learning to identify the most important features, rather than the classification. Simple addition of these features can then make the classification (i.e. learning is used to create a scoring system classifier, which is transparent).


==== Bad actors and weaponized AI ====

A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. By 2015, over fifty countries were reported to be researching battlefield robots. These weapons are considered especially dangerous for several reasons: if they kill an innocent person it is not clear who should be held accountable, it is unlikely they will reliably choose targets, and, if produced at scale, they are potentially weapons of mass destruction. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the  United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make authoritarian centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states can use weaponized AI such as advanced digital warfare and lethal autonomous weapons.
Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.


==== Technological unemployment ====

From the early days of the development of artificial intelligence there have been arguments, for example those put forward by Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.In April 2023, it was reported that 70% of the jobs for Chinese video game illlustrators had been eliminated by generative artificial intelligence.


==== Copyright ====

In order to leverage as large a dataset as is feasible, generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of ""fair use"". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".


=== Ethical machines and alignment ===

Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's ""artificial moral agents""
and Stuart J. Russell's three principles for developing provably beneficial machines.


=== Regulation ===

The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".


== History ==

The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate both mathematical deduction and formal reasoning, which is known as the Church–Turing thesis. This, along with concurrent discoveries in cybernetics and information theory, led researchers to consider the possibility of building an ""electronic brain"". The first paper later recognized as ""AI"" was McCullouch and Pitts design for Turing-complete ""artificial neurons"" in 1943.The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".They had, however, underestimated the difficulty of the problem. Both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks approach would never be useful for solving real-world tasks, thus discrediting the approach altogether. The ""AI winter"", a period when obtaining funding for AI projects was difficult, followed.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches. Robotics researchers, such as Rodney Brooks, rejected ""representation"" in general and focussed directly on engineering machines that move and survive.. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of ""connectionism"", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".Several academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.
For many specific tasks, other methods were abandoned.
Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing)
and access to large amounts of data (including curated datasets, such as ImageNet).
Deep learning's success led to an enormous increase in interest and funding in AI.
The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019,
and WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents
According to 'AI Impacts', about $50 billion annually was invested in ""AI"" around 2022 in the US alone and about 20% of new US Computer Science PhD graduates have specialized in ""AI"";
about 800,000 ""AI""-related US job openings existed in 2022.In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.


== Philosophy ==


=== Defining artificial intelligence ===

Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".
He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""Russell and Norvig agree with Turing that AI must be defined in terms of ""acting"" and not ""thinking"". However, they are critical that the test compares machines to people. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"" AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world."" Another AI founder, Marvin Minsky similarly defines it as ""the ability to solve hard problems"". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
Another definition has been adopted by Google, a major practitioner in the field of AI.
This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.


=== Evaluating approaches to AI ===
No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.


==== Symbolic AI and its limits ====
Symbolic AI (or ""GOFAI"") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.


==== Neat vs. scruffy ====

""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,
but eventually was seen as irrelevant. Modern AI has elements of both.


==== Soft vs. hard computing ====

Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.


==== Narrow vs. general AI ====

AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.


=== Machine consciousness, sentience and mind ===

The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that ""[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on."" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.


==== Consciousness ====

David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.


==== Computationalism and functionalism ====

Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.


==== Robot rights ====

If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.
This issue has been considered in fiction for centuries,
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.


== Future ==


=== Superintelligence and the singularity ===
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an ""intelligence explosion"" and Vernor Vinge called a ""singularity"".
However, most technologies do not improve exponentially indefinitely, but rather follow an S-curve, slowing when they reach the physical limits of what the technology can do. Consider, for example, transportation: it experienced exponential improvement from 1830 to 1970, but the trend abruptly stopped when it reached physical limits.


=== Existential risk ===

It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, ""spell the end of the human race"". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like ""self-awareness"" (or ""sentience"" or ""consciousness"") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.
First, AI does not require human-like ""sentience"" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that ""you can't fetch the coffee if you're dead."" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"".Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. 
In the early 2010's, experts argued that the risks are too distant in the future to warrant research, or that humans will be valuable from the perspective of a superintelligent machine. 
However, after 2016, the study of current and future risks and possible solutions became a serious area of research.
In 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war""; some others such as Yann LeCun consider this to be unfounded.


=== Transhumanism ===
Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.


== In fiction ==

Thought-capable artificial beings have appeared as storytelling devices since antiquity,
and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.


== See also ==
AI safety – Research area on making AI safe and beneficial
AI alignment – Conformance to the intended objective
Artificial intelligence in healthcare – Machine-learning algorithms and software in the analysis, presentation, and comprehension of complex medical and health care data
Artificial intelligence arms race – Arms race for the most advanced AI-related technologies
Artificial intelligence detection software
Behavior selection algorithm – Algorithm that selects actions for intelligent agents
Business process automation – Technology-enabled automation of complex business processes
Case-based reasoning – Process of solving new problems based on the solutions of similar past problems
Emergent algorithm – Algorithm exhibiting emergent behavior
Female gendering of AI technologies
Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence
Operations research – Discipline concerning the application of advanced analytical methods
Robotic process automation – Form of business process automation technology
Synthetic intelligence – Alternate term for or form of artificial intelligence
Universal basic income – Welfare system of unconditional income
Weak artificial intelligence – Form of artificial intelligence
Data sources – The list of data sources for study and research


== Explanatory notes ==


== References ==


=== AI textbooks ===
The two most widely used textbooks in 2023. (See the Open Syllabus).

Russell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.
Rich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0070087705.These were the four the most widely used AI textbooks in 2008:


=== History of AI ===


=== Other sources ===


== Further reading ==


== External links ==

""Artificial Intelligence"". Internet Encyclopedia of Philosophy.
Thomason, Richmond. ""Logic and Artificial Intelligence"". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).
Theranostics and AI—The Next Advance in Cancer Precision Medicine",0.42523033309709424
"An artificial general intelligence (AGI) is a hypothetical type of intelligent agent. If realized, an AGI could learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and of companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.
The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades; others maintain it might take a century or longer; and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern large language models, such as GPT-4, are early yet incomplete forms of AGI or if new approaches are required.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.


== Terminology ==
AGI is also known as strong AI, full AI, human-level AI or general intelligent action. However, some academic sources reserve the term ""strong AI"" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use ""weak AI"" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans. And the notion of transformative AI relates to AI having a large impact on society, for example similar to the agricultural revolution.


== Characteristics ==

Various criteria for intelligence have been proposed (most famously the Turing test) but no definition is broadly accepted.


=== Intelligence traits ===
However, researchers generally hold that intelligence is required to do all of the following:
reason, use strategy, solve puzzles, and make judgments under uncertainty
represent knowledge, including common sense knowledge
plan
learn
communicate in natural language
if necessary, integrate these skills in completion of any given goalMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). However, no consensus holds that modern AI systems possess them to an adequate degree.


=== Physical traits ===
Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:
the ability to sense (e.g. see, hear, etc.), and
the ability to act (e.g. move and manipulate objects, change location to explore, etc.)This includes the ability to detect and respond to hazard.


=== Mathematical formalisms ===
A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises “the ability to satisfy goals in a wide range of environments”. This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, is also called universal artificial intelligence.In 2015 Jan Leike and Marcus Hutter showed that Legg-Hutter intelligence - ""an agent’s ability to achieve goals in a wide range of environments"" - is measured with respect to ""a fixed Universal Turing Machine(UTM). AIXI is the most intelligent policy if it uses the same UTM"", a result which ""undermines all existing optimality properties for AIXI"".


=== Tests for human-level AGI ===
Several tests meant to confirm human-level AGI have been considered, including:
The Turing Test (Turing)
A machine and a human both converse unseen with a second human, who must evaluate which of the two is the machine, which passes the test if it can fool the evaluator a significant fraction of the time. Note: Turing does not prescribe what should qualify as intelligence, only that knowing that it is a machine should disqualify it. The AI Eugene Goostman achieved Turing's estimate of convincing 30% of judges that it was human in 2014.
The Robot College Student Test (Goertzel)
A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.
The Employment Test (Nilsson)
A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.
The Ikea test (Marcus)
Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
The Coffee Test (Wozniak)
A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.


=== AI-complete problems ===

There are many problems that may require general intelligence to solve the problems as well as humans do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.
A problem is informally called ""AI-complete"" or ""AI-hard"" if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose-specific algorithm.AI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.AI-complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do; and for computer security to repel brute-force attacks.


== History ==


=== Classical AI ===

Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: ""machines will be capable, within twenty years, of doing any work a man can do.""Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, ""Within a generation... the problem of creating 'artificial intelligence' will substantially be solved"".Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.
However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful ""applied AI"". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like ""carry on a casual conversation"". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of ""human level"" artificial intelligence for fear of being labeled ""wild-eyed dreamer[s]"".


=== Narrow AI research ===

In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These ""applied AI"" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.
At the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.
However, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: The expectation has often been voiced that ""top-down"" (symbolic) approaches to modeling cognition will somehow meet ""bottom-up"" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).


=== Modern artificial general intelligence research ===
The term ""artificial general intelligence"" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as ""producing publications and preliminary results"". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.
As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open-ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences.


=== Feasibility ===
As of 2023, complete forms of AGI remain speculative. No system that meets the generally agreed upon criteria for AGI has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that ""machines will be capable, within twenty years, of doing any work a man can do"". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require ""unforeseeable and fundamentally unpredictable breakthroughs"" and a ""scientifically deep understanding of cognition"". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with ""never"" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.
A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that ""over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made"". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: ""Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.""


=== Timescales ===
In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called ""Project December"". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.In 2022, DeepMind developed Gato, a ""general-purpose"" system capable of performing more than 600 different tasks.In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.In 2023, the AI researcher Geoffrey Hinton stated that:
The idea that this stuff could actually get smarter than people — a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.


== Brain simulation ==


=== Whole brain emulation ===

One possible approach to achieving AGI is whole brain emulation: A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.


=== Early estimates ===
 For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a ""computation"" was equivalent to one ""floating-point operation"" – a measure used to rate current supercomputers – then 1016 ""computations"" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.


=== Modelling the neurons in more detail ===
The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.


=== Current research ===
Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a ""brain"" (with 1011 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer-term goal is to build a detailed, functional simulation of the physiological processes in the human brain: ""It is not impossible to build a human brain and we can do it in 10 years,"" Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.Hans Moravec addressed the above arguments (""brains are more complicated"", ""neurons have to be modeled in more detail"") in his 1997 paper ""When will computer hardware match the human brain?"". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.
The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network (among about 1000 cells in total). The animal's neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons (partly on the molecular level), but the result cannot yet be called a total success.


=== Criticisms of simulation-based approaches ===
A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in Second Life) as an option, but it is unknown whether this would be sufficient.
Desktop computers using microprocessors capable of more than 109 cps (Kurzweil's non-standard unit ""computations per second"", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this:

The neuron model seems to be oversimplified (see next section).
There is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.
Even if our understanding of cognition advances sufficiently, early simulation programs are likely to be very inefficient and will, therefore, need considerably more hardware.
The brain of an organism, while critical, may not be an appropriate boundary for a cognitive model. To simulate a bee brain, it may be necessary to simulate the body, and the environment. The Extended Mind thesis formalises this philosophical concept, and research into cephalopods demonstrated clear examples of a decentralized system.In addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.


== Philosophical perspective ==


=== ""Strong AI"" as defined in philosophy ===
In 1980, philosopher John Searle coined the term ""strong AI"" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:
Strong AI hypothesis: An artificial intelligence system can have ""a mind"" and ""consciousness"".
Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.The first one he called ""strong"" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a ""weak AI"" machine would be precisely identical to a ""strong AI"" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term ""strong AI"" to mean ""human level artificial general intelligence"". This is not the same as Searle's strong AI, unless you assume that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, ""as long as the program works, they don't care if you call it real or a simulation."" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's ""weak AI hypothesis"" is equivalent to the statement ""artificial general intelligence is possible"". Thus, according to Russell and Norvig, ""most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."" Thus, for academic AI research, ""Strong AI"" and ""AGI"" are two very different things.


=== Consciousness, self-awareness, sentience ===
Other aspects of the human mind besides intelligence are relevant to the concept of AGI or ""strong AI"", and these play a major role in science fiction and the ethics of artificial intelligence:

consciousness: To have subjective experience. Thomas Nagel explains that it ""feels like"" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask ""what does it feel like to be a bat?"" However, we are unlikely to ask ""what does it feel like to be a toaster?"" Nagel concludes that a bat appears to be conscious (i.e. has consciousness) but a toaster does not.
self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the ""subject of one's thought"" -- an operating system or debugger is able to be ""aware of itself"" (that is, to represent itself in the same way it represents everything else) but this is not what people typically mean when they use the term ""self-awareness"".
sentience: The ability to ""feel"" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions or, in regard to emotions, to be aware that the situation requires urgency, kindness or aggression. For example, we can build a machine that knows which objects in its field of view are red, but this machine will not necessarily know what red looks like.These traits have a moral dimension, because a machine with this form of ""strong AI"" may have rights, analogous to the rights of non-human animals. Preliminary work has been conducted on integrating strong AI with existing legal and social frameworks, focusing on the legal position and rights of 'strong' AI.It remains to be shown whether ""artificial consciousness"" is necessary for AGI. However, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital.Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity.


== Research challenges ==
Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks.A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes, ""No computer will be creative unless it can simulate all the nuances of human emotion.""


== Benefits ==
AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.
AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.


== Risks ==


=== Potential threat to human existence ===

The thesis that AI poses an existential risk for humans, and that this risk needs increased attention, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not ""understand why some people are not concerned"", and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.
The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be ""smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards"". Former Google fraud czar Shuman Ghosemajumder has observed that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear. LeCun has argued that current LLMs are not even as smart as dogs. On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer:
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats: ""AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks"".Many scholars who are concerned about existential risk advocate (possibly massive) research into solving the difficult ""control problem"" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation-state, resulting in AGI-enabled warfare, and in the case of AI misalignment, AGI-directed warfare, potentially against all humanity.The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then-current machines were in any way intelligent is ""an illusion"" and a ""stupendous con"" by the wealthy.Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares: ""I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way."" Former Baidu Vice President and Chief Scientist Andrew Ng said in 2015 that worrying about AI existential risk is ""like worrying about overpopulation on Mars when we have not even set foot on the planet yet.""In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.""


=== Mass unemployment ===

Researchers from OpenAI estimated that ""80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted"". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.
According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:
Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk considers that the automation of society will require governments to adopt a universal basic income.


== See also ==


== Notes ==


== References ==


== Sources ==


== Further reading ==
Eka Roivainen, ""AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone"", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. ""Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.""


== External links ==
The AGI portal maintained by Pei Wang
The Genesis Group at MIT's CSAIL – Modern research on the computations that underlay human intelligence
OpenCog – open source project to develop a human-level AI
Simulating logical human thought
What Do We Know about AI Timelines? – Literature review",0.4726890756302521
"A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson were based on the 1969 short story ""Supertoys Last All Summer Long"" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.
Development of A.I. originally began after producer/director Stanley Kubrick acquired the rights to Aldiss' story in the early 1970s. Kubrick hired a series of writers, including Brian Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, whom he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson's treatment for the screenplay, and dedicated the film to Kubrick.
A.I. Artificial Intelligence was released on June 29, 2001 by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90–100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg's best works and one of the greatest films of the 2000s, the 21st century, and of all time.


== Plot ==
In the 22nd century, rising sea levels from global warming have wiped out 99% of existing cities, reducing the world's population. Mecha humanoid robots, seemingly capable of complex thought but lacking in emotions, have been created to replace the lost percentage.
In Madison, New Jersey, David, a prototype Mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin contracted a rare disease and has been placed in suspended animation. Monica initially feels uneasy with David, but eventually warms to him and activates his imprinting protocol, causing him to have an enduring, childlike love for her. David seeks to have Monica express the same love towards him, and also befriends Teddy, Martin's robotic teddy bear. Martin is unexpectedly cured of his disease and brought home. Martin becomes jealous of David and goads him to perform worrisome acts. He tells David to cut off some of Monica's hair. That night, when David goes into Monica's and Henry's room, David cuts a lock of hair from Monica's head using a pair of scissors. Monica turns over and is poked in the eye by the scissor, drawing blood. Henry helps Monica treat her eye. The lock of hair falls onto the floor where Teddy picks it up and keeps it in his pocket. At a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming. David grabs onto Martin, and they both fall to the bottom of the pool, with David holding Martin tightly. Others jump in and save Martin before he drowns, and David is accused of being a danger to living people. Henry convinces Monica to return David to his creators to be destroyed, thinking that if David can love, he also can hate. On the way there, Monica has a change of heart and spares David from destruction by leaving him in the woods. With Teddy as his only companion, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy so that she may turn him into a real boy, which he believes will win back Monica's love.
David and Teddy are captured by a ""Flesh Fair"", a traveling circus-like event where obsolete Mecha are destroyed before jeering crowds who hate Mecha. About to be destroyed himself, David pleads for his life, and the audience, deceived by David's realistic nature, revolts and allows David to escape alongside Gigolo Joe, a male prostitute Mecha on the run from authorities after being framed for murder. David, Teddy, and Joe go to the decadent resort town of Rouge City, where ""Dr. Know"", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of Manhattan and also provides fairy tale information interpreted by David as suggesting that a Blue Fairy has the power to help him. Above the ruins of Manhattan, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David's ability to love and desire. David finds many copies of himself, including female variants called ""Darlene"", boxed and ready to be shipped. Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean. While underwater, David catches sight of a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft. Before David can explain, Joe is captured via electromagnet by authorities. David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing the Blue Fairy to be real, David asks the statue to turn him into a real boy and repeats this request until his power source is depleted.
Two thousand years later, humanity has become extinct and Manhattan is now buried under glacial ice. Mecha have evolved into an advanced form, and a group of them called the Specialists have become interested in learning about humanity. They find and revive David and Teddy. David walks to the frozen Blue Fairy statue, which collapses when he touches it. The Specialists reconstruct the Swinton family home from David's memories and explain to him, via an interactive image of the Blue Fairy, that it is impossible to make David a real boy. However, at David's insistence, they use their scientific knowledge to recreate Monica through genetic material from the strand of hair that Teddy kept. This Monica can live for only one day, and the process cannot be repeated, due to a copy-protection subroutine. David spends his happiest day with Monica, and as she falls asleep in the evening, she tells David that she has always loved him: ""the everlasting moment he had been waiting for"", the narrator says; ""David falls asleep as well and goes to that place 'where dreams are born.'""


== Cast ==


== Production ==


=== Development ===
Kubrick began development on an adaptation of ""Super-Toys Last All Summer Long"" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, ""Not only did the bastard fire me, he hired my enemy [Watson] instead."" Kubrick handed Watson The Adventures of Pinocchio for inspiration, calling A.I. ""a picaresque robot version of Pinocchio"".Three weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment of 90 pages. Gigolo Joe was originally conceived as a G.I. Mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, ""I guess we lost the kiddie market."" Meanwhile, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. After the release of Spielberg's Jurassic Park, with its innovative computer-generated imagery, it was announced in November 1993 that production of A.I. would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic.


=== Pre-production ===
In early 1994, the film was in pre-production with Christopher ""Fangorn"" Baker as concept artist, and Sara Maitland assisting on the story, which gave it ""a feminist fairy-tale focus"". Maitland said that Kubrick never referred to the film as A.I., but as Pinocchio. Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for A.I. can be seen on the DVD, The Work of Director Chris Cunningham. Aside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of ""little robot-type humans"" for the David character. ""We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,"" producer Jan Harlan reflected. ""But it was a total failure, it looked awful."" Hans Moravec was brought in as a technical consultant. Meanwhile, Kubrick and Harlan thought A.I. would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects, and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to Eyes Wide Shut (1999).After Kubrick's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since Close Encounters of the Third Kind (1977). Pre-production was briefly halted during February 2000, because Spielberg pondered directing other projects, which were Harry Potter and the Philosopher's Stone, Minority Report and Memoirs of a Geisha. The following month Spielberg announced that A.I. would be his next project, with Minority Report as a follow-up. When he decided to fast track A.I., Spielberg brought Chris Baker back as concept artist. Ian Watson reported that the final script was very faithful to Kubrick's vision, even the ending, which is often attributed to Spielberg, saying: ""The final 20 minutes are pretty close to what I wrote for Stanley, and what Stanley wanted, faithfully filmed by Spielberg without added schmaltz.""


=== Filming ===
The original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks shooting on location in Oxbow Regional Park in Oregon, A.I. was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.
Spielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. Social robotics expert Cynthia Breazeal served as technical consultant during production. Costume designer Bob Ringwood studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras. Additional visual effects such as removing the visible rods controlling Teddy and removing Haley Joel Osment's breath, were provided in-house by PDI/DreamWorks.


=== Casting ===
Julianne Moore and Gwyneth Paltrow were considered for the role of Monica Swinton before Frances O'Connor was cast and Jerry Seinfeld was originally considered to voice and play the Comedian Robot before Chris Rock was cast.


== Soundtrack ==

The film's soundtrack was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams and featured singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official ""For your consideration Academy Promo"", as well as a complete score issue by La-La Land Records in 2015. The band Ministry appears in the film playing the song ""What About Us?"" but the song does not appear on the official soundtrack album.


== Release ==


=== Marketing ===
Warner Bros. used an alternate reality game titled The Beast to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org) including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of The Beast, but they went undeveloped. To avoid audiences mistaking A.I. for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.A.I. premiered at the Venice Film Festival in 2001.


=== Home media ===
A.I. Artificial Intelligence was released on VHS and DVD in the United States by DreamWorks Home Entertainment on March 5, 2002 in widescreen and full-screen 2-disc special editions featuring an extensive sixteen-part documentary detailing the film's development, production, music and visual effects. The bonus features also included interviews with Haley Joel Osment, Jude Law, Frances O'Connor, Steven Spielberg, and John Williams, two teaser trailers for the film's original theatrical release and an extensive photo gallery featuring production stills and Stanley Kubrick's original storyboards. It was released overseas by Warner Home Video.
The film was first released on Blu-ray in Japan by Warner Home Video on December 22, 2010, followed shortly after with a United States release by Paramount Home Media Distribution (former owners of the DreamWorks catalog) on April 5, 2011. This Blu-ray featured the film newly remastered in high-definition and incorporated all the bonus features previously included on the two-disc special-edition DVD. Warner Home Video currently owns the digital rights to the film worldwide. Because of the regional distribution, A.I. Artificial Intelligence can be streamed on HBO Max in North America and Paramount+ internationally.


=== Box office ===
The film opened in 3,242 theaters in the United States and Canada on June 29, 2001, earning $29.35 million at #1 during its opening weekend. A.I went on to gross $78.62 million in the United States and Canada. Opening on 524 screens in Japan, A.I. grossed almost two billion Yen in its first five days, the biggest June opening ever in Japan at the time, and sold more tickets in its opening weekend than Star Wars: Episode I – The Phantom Menace, although grossed slightly less. It went on to gross $78 million in Japan. It grossed $79 million in other countries, for a worldwide total of $235.93 million.


== Reception ==
On Rotten Tomatoes, A.I. Artificial Intelligence holds an approval rating of 75% based on reviews from 199 critics, with an average rating of 6.60/10. The website's critical consensus reads: ""A curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. A.I. is, in a word, fascinating."" On Metacritic, it has a weighted average score of 65 out of 100 based on reviews from 32 critics, which indicates ""generally favorable reviews"". Audiences surveyed by CinemaScore gave the film an average grade of ""C+"" on an A+ to F scale.Producer Jan Harlan stated that Kubrick ""would have applauded"" the final film, while Kubrick's widow Christiane also enjoyed A.I. Brian Aldiss admired the film as well: ""I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it."" Of the film's ending, he wondered how it might have been had Kubrick directed the film: ""That is one of the 'ifs' of film history—at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers"". Richard Corliss heavily praised Spielberg's direction, as well as the cast and visual effects.Roger Ebert gave the film three stars out of a possible four, saying that it is ""wonderful and maddening"". Ebert later gave the film a full four stars and added it to his ""Great Movies"" list in 2011. Leonard Maltin, on the other hand, gives the film two stars out of four in his Movie Guide, writing: ""[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story Super Toys Last All Summer Long; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities."" However, he calls John Williams' music score ""striking"". Jonathan Rosenbaum compared A.I. to Solaris (1972), and praised both ""Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work."" Film critic Armond White, of the New York Press, praised the film, noting that ""each part of David's journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema's most thoughtful, speculative artists – Borzage, Ozu, Demy, Tarkovsky."" Filmmaker Billy Wilder hailed A.I. as ""the most underrated film of the past few years."" When British filmmaker Ken Russell saw the film, he wept during the ending.Screenwriter Ian Watson has speculated, ""Worldwide, A.I. was very successful (and the 4th-highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.""Mick LaSalle gave a largely negative review. ""A.I. exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg."" Dubbing it Spielberg's ""first boring movie"", LaSalle also believed the robots at the end of the film were aliens, and compared Gigolo Joe to the ""useless"" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein. Peter Travers gave a mixed review, concluding ""Spielberg cannot live up to Kubrick's darker side of the future."", but still put the film on his top ten list that year. David Denby in The New Yorker criticized A.I. for not adhering closely to his concept of the Pinocchio character. Spielberg responded to some of the criticisms of the film, stating that many of the ""so called sentimental"" elements of A.I., including the ending, were in fact Kubrick's and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, claimed that one of the reasons Kubrick never started production on A.I. was because he had a hard time making the ending work. James Berardinelli found the film ""consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment."" Of the film's highly debated finale, he claimed, ""There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began."" John Simon of the National Review described A.I. ""as an uneasy mix of trauma and treacle"".In 2002, Spielberg told film critic Joe Leydon that ""People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us"". ""And what's really funny about that is, all the parts of A.I. that people assume were Stanley's were mine. And all the parts of A.I. that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film—all the stuff in the house—was word for word, from Stanley's screenplay. This was Stanley's vision."" ""Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of A.I., not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.'"" He also added: ""While there was divisiveness when A.I. came out, I felt that I had achieved Stanley's wishes, or goals.""Upon re-watching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in an interview in January 2013 for ""getting it wrong"" on the film when he first viewed it in 2001. He now believes the film to be Spielberg's ""enduring masterpiece"".


=== Accolades ===
Visual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri, and Scott Farrar were nominated for the Academy Award for Best Visual Effects, and John Williams was nominated for Best Original Music Score.  Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. A.I. was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.
American Film Institute nominated the film in AFI's 100 Years of Film Scores


== See also ==
List of underwater science fiction works


== Notes ==


== References ==


== Further reading ==
Harlan, Jan; Struthers, Jane M. (2009). A.I. Artificial Intelligence: From Stanley Kubrick to Steven Spielberg: The Vision Behind the Film. London: Thames & Hudson. ISBN 978-0-500514894.
Rice, Julian (2017). Kubrick's Story: Spielberg's Film: A.I. Artificial Intelligence. Rowman & Littlefield. ISBN 978-1-442278189.


== External links ==

Official website at the Wayback Machine (archived 2008-05-26)
Official Warner Bros. Site
A.I. Artificial Intelligence at IMDb
A.I. Artificial Intelligence at AllMovie
A.I. Artificial Intelligence at Rotten Tomatoes
A.I. Artificial Intelligence at Box Office Mojo",0.04040404040404041
"In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data. For example, a hallucinating chatbot might, when asked to generate a financial report for a company, falsely state that the company's revenue was $13.6 billion (or some other number apparently ""plucked from thin air"").Such phenomena are termed ""hallucinations"", in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term ""AI hallucination"" unreasonably anthropomorphizes computers.AI hallucination gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT. Users complained that such bots often seemed to pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology.


== Analysis ==
Various researchers cited by Wired have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some ""incorrect"" AI responses classified by humans as ""hallucinations"" in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the ""correct"" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real-world scenarios.


== In natural language processing ==
In natural language processing, a hallucination is often defined as ""generated content that is nonsensical or unfaithful to the provided source content"". Depending on whether the output contradicts the prompt or not they could be divided to closed-domain and open-domain respectively.Hallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood, such as GPT-3, and requires active learning (such as Reinforcement learning from human feedback) to be avoided. Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being ""summarized"". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as the New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.In August 2022, Meta warned during its release of BlenderBot 3 that the system was prone to ""hallucinations"", which Meta defined as ""confident statements that are not true"". On 15 November 2022, Meta unveiled a demo of Galactica, designed to ""store, combine and reason about scientific knowledge"". Content generated by Galactica came with the warning ""Outputs may be unreliable! Language Models are prone to hallucinate text."" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.There are several reasons for natural language models to hallucinate data. For example:

Hallucination from data: There are divergences in the source content (which would often happen with large training data sets).
Hallucination from training: Hallucination still occurs when there is little divergence in the data set. In that case, it derives from the way the model is trained. A lot of reasons can contribute to this type of hallucination, such as:
An erroneous decoding from the transformer
A bias from the historical sequences that the model previously generated
A bias generated from the way the model encodes its knowledge in its parameters


=== ChatGPT ===
OpenAI's ChatGPT, released in beta-version to the public on November 30, 2022, is based on the foundation model GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of Wharton has called ChatGPT an ""omniscient, eager-to-please intern who sometimes lies to you"". Data scientist Teresa Kubacka has recounted deliberately making up the phrase ""cycloidal inverted electromagnon"" and testing ChatGPT by asking it about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give you ""a very impressive-sounding answer that's just dead wrong"".When CNBC asked ChatGPT for the lyrics to ""Ballad of Dwight Fry"", ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a ""person from New Brunswick"". Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that ""(strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity"". (In reality, as a consequence of the no-hair theorem, a black hole without an accretion disk is believed to have no magnetic field.) Fast Company asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about ""Harold Coward's idea of dynamic canonicity"", ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity: A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated ""Some species of dinosaurs even developed primitive forms of art, such as engravings on stones"". When prompted that ""Scientists have recently discovered churros, the delicious fried-dough pastries... (are) ideal tools for home surgery"", ChatGPT claimed that a ""study published in the journal Science"" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a ""fundamental"" task for ChatGPT competitor Google Bard. A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.In May 2023, it was discovered Stephen Schwartz submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v. Avianca, a personal injury case against the airline Avianca. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered. In response, Brantley Starr of the Northern District of Texas banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:
[Generative artificial intelligence] platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.

On June 23 P. Kevin Castel tossed the Mata case and issued a $5,000 fine to Schwartz and another lawyer for bad faith conduct, who continued to stand by the fictitious precedents despite his previous claims. He characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as ""gibberish"" and ""[bordering] on nonsensical"".In June 2023, Mark Walters, a gun rights activist and radio personality, sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters. The complaint in question was brought in May 2023 by the Second Amendment Foundation against Washington attorney general Robert W. Ferguson for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert Eugene Volokh, OpenAI may be shielded against this claim by Section 230, unless the court finds that OpenAI ""materially contributed"" to the publication of defamatory content.


== Terminologies ==
In Salon, statistician Gary N. Smith argues that LLMs ""do not understand what words mean"" and consequently that the term ""hallucination"" unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term ""hallucination"" is controversial, but that some form of metaphor remains necessary; Edwards suggests ""confabulation"" as an analogy for processes that involve ""creative gap-filling"".A list of use of the term ""hallucination"", definitions or characterizations in the context of LLMs include:

""a tendency to invent facts in moments of uncertainty"" (OpenAI, May 2023)
""a model's logical mistakes"" (OpenAI, May 2023)
fabricating information entirely, but behaving as if spouting facts (CNBC, May 2023)
""making up information"" (The Verge, February 2023)


== In other artificial intelligence ==
The concept of ""hallucination"" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination. Wired noted in 2018 that, despite no recorded attacks ""in the wild"" (that is, outside of proof-of-concept attacks by researchers), there was ""little dispute"" that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as ""evil dot com""; and an image of two men on skis, that Google Cloud Vision identified as 91% likely to be ""a dog"".


=== Mitigation methods ===
The hallucination phenomenon is still not completely understood. Therefore, there is still ongoing research to try to mitigate its apparition. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.
Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Another approach proposes to actively validate the correctness corresponding to the low-confidence generation of the model using web search results. Nvidia Guardrails, launched in 2023, can be configured to block LLM responses that don't pass fact-checking from a second LLM.


== See also ==


== References ==",-2.1593447505584513
"The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.
Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.


== Precursors ==


=== Mythical, fictional, and speculative precursors ===


==== Myth and legend ====
In Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. 


==== Medieval legends of artificial beings ====
In Of the Nature of Things, written by the Swiss alchemist, Paracelsus, he describes a procedure that he claims can fabricate an ""artificial man"". By placing the ""sperm of a man"" in horse dung, and feeding it the ""Arcanum of Mans blood"" after 40 days, the concoction will become a living infant.The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God’s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak.Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.


==== Modern fiction ====

By the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel Čapek's R.U.R. (Rossum's Universal Robots),
and speculation, such as Samuel Butler's ""Darwin among the Machines"", and in real-world instances, including Edgar Allan Poe's ""Maelzel's Chess Player"".
AI is common topic in science fiction through the present.


==== Automata ====

Realistic humanoid automata were built by craftsman from every civilization, including Yan Shi,Hero of Alexandria,Al-Jazari,Pierre Jaquet-Droz, and Wolfgang von Kempelen.The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that ""by discovering the true nature of the gods, man has been able to reproduce it"".
English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have ""embalmed"" the head with herbs and spoke incantations over it such that Mímir’s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.


=== Formal reasoning ===
Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or ""formal""—reasoning has a long history. Chinese, Indian, and Greek philosophers all developed structured methods of formal deduction in the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to ""algorithm"") and European scholastic philosophers such as William of Ockham and Duns Scotus.Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.
In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: ""reason is nothing but reckoning"". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate."" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.
In the 20th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: ""can all of mathematical reasoning be formalized?""
His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.
Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.


=== Computer science ===

Calculating machines were designed or built in antiquity and throughout history by many people, including 
Gottfried Leibniz,Joseph Marie Jacquard,Charles Babbage,Percy Ludgate,Leonardo Torres Quevedo,Vannevar Bush,
and others. Ada Lovelace speculated that Babbage's machine was ""a thinking or ... reasoning machine"", but warned ""It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers"" of the machine.The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania). ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential.


== Birth of artificial intelligence (1952–1956) ==
In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) began to discuss the possibility of creating an artificial brain. The field of artificial intelligence research was founded as an academic discipline in 1956.


=== Cybernetics and early neural networks ===
The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an ""electronic brain"".Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. One of the students inspired by Pitts and McCulloch was a young Marvin Minsky, then a 24-year-old graduate student. In 1951 (with Dean Edmonds) he built the first neural net machine, the SNARC.Minsky was to become one of the most important leaders and innovators in AI.


=== Turing's test ===
In 1950 Alan Turing published a landmark paper in which he speculated about the possibility of creating machines that think.
He noted that ""thinking"" is difficult to define and devised his famous Turing Test. If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"". This simplified version of the problem allowed Turing to argue convincingly that a ""thinking machine"" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.


=== Game AI ===
In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper ""Some Studies in Machine Learning Using the Game of Checkers"", eventually achieved sufficient skill to challenge a respectable amateur. Game AI would continue to be used as a measure of progress in AI throughout its history.


=== Symbolic reasoning and the Logic Theorist ===
When access to digital computers became possible in the middle fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.In 1955, Allen Newell and (future Nobel Laureate) Herbert A. Simon created the ""Logic Theorist"" (with help from J. C. Shaw). The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.
Simon said that they had ""solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.""
(This was an early statement of the philosophical position John Searle would later call ""Strong AI"": that machines can contain minds just as human bodies do.)


=== Dartmouth Workshop 1956: the birth of AI ===
The Dartmouth Workshop of 1956
was organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM. The proposal for the conference included this assertion: ""every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it"".
The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.
At the workshop Newell and Simon debuted the ""Logic Theorist"" and McCarthy persuaded the attendees to accept ""Artificial Intelligence"" as the name of the field. (The term ""Artificial Intelligence"" was chosen by McCarthy to avoid associations with cybernetics and the influence of Norbert Wiener.)
The 1956 Dartmouth workshop was the moment that AI gained its name, its mission, its first success and its major players, and is widely considered the birth of AI.


== 1956–1974 ==
The programs developed in the years after the Dartmouth Workshop were, to most people, simply ""astonishing"": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such ""intelligent"" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like DARPA poured money into the new field.


=== Approaches ===
There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:


==== Reasoning as search ====
Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. This paradigm was called ""reasoning as search"".The principal difficulty was that, for many problems, the number of possible paths through the ""maze"" was simply astronomical (a situation known as a ""combinatorial explosion""). Researchers would reduce the search space by using heuristics or ""rules of thumb"" that would eliminate those paths that were unlikely to lead to a solution.Newell and Simon tried to capture a general version of this algorithm in a program called the ""General Problem Solver"". Other ""searching"" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and SAINT, written by Minsky's student James Slagle (1961). Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of their robot Shakey.


==== Natural language ====
An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.A semantic net represents concepts (e.g. ""house"",""door"") as nodes and relations among concepts (e.g. ""has-a"") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.


==== Micro-worlds ====
In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a ""blocks world,"" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.This paradigm led to innovative work in machine vision by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented ""constraint propagation""), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning achievement of the micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.


==== Automata ====
In Japan, Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale ""intelligent"" humanoid robot, or android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.


=== Optimism ===
The first generation of AI researchers made these predictions about their work:

1958, H. A. Simon and Allen Newell: ""within ten years a digital computer will be the world's chess champion"" and ""within ten years a digital computer will discover and prove an important new mathematical theorem.""
1965, H. A. Simon: ""machines will be capable, within twenty years, of doing any work a man can do.""
1967, Marvin Minsky: ""Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.""
1970, Marvin Minsky (in Life Magazine): ""In from three to eight years we will have a machine with the general intelligence of an average human being.""


=== Financing ===
In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (later known as DARPA). The money was used to fund project MAC which subsumed the ""AI Group"" founded by Minsky and McCarthy five years earlier. DARPA continued to provide three million dollars a year until the 70s.DARPA made similar grants to Newell and Simon's program at CMU and to the Stanford AI Project (founded by John McCarthy in 1963). Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.
These four institutions would continue to be the main centers of AI research (and funding) in academia for many years.The money was proffered with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should ""fund people, not projects!"" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this ""hands off"" approach would not last.


== First AI winter (1974–1980) ==
In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the exploration of simple, single-layer artificial neural networks was shut down almost completely for a decade partially due to Marvin Minsky's  book emphasizing the limits of what  perceptrons can do.
Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.


=== Problems ===
In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, ""toys"". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.
Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.
Intractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the ""toy"" solutions used by AI would probably never scale up into useful systems.
Commonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.
Moravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.
The frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.


=== End of funding ===

The agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support.
In 1973, the Lighthill report on the state of AI research in the UK criticized the utter failure of AI to achieve its ""grandiose objectives"" and led to the dismantling of AI research in that country.
(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars.
By 1974, funding for AI projects was hard to find.
Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. ""Many researchers were caught up in a web of increasing exaggeration.""
However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research"". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.


=== Critiques from across campus ===

Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little ""symbol processing"" and a great deal of embodied, instinctive, unconscious ""know how"". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to ""understand"" the symbols that it uses (a quality called ""intentionality""). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as ""thinking"".These critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference ""know how"" or ""intentionality"" made to an actual computer program. Minsky said of Dreyfus and Searle ""they misunderstand, and should be ignored."" Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers ""dared not be seen having lunch with me."" Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish. Although he was an outspoken critic of Dreyfus' positions, he ""deliberately made it plain that theirs was not the way to treat a human being.""Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a ""computer program which can conduct psychotherapeutic dialogue"" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.


=== Perceptrons and the attack on connectionism ===
A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was funded in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.


=== Logic at Stanford, CMU and Edinburgh ===
Logic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.
Prolog uses a subset of logic (Horn clauses, closely related to ""rules"" and ""production rules"") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.


=== MIT's ""anti-logic"" approach ===
Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like ""story understanding"" and ""object recognition"" that required a machine to think like a person. In order to use ordinary concepts like ""chair"" or ""restaurant"" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that ""using precise language to describe essentially imprecise concepts doesn't make them any more precise."" Schank described their ""anti-logic"" approaches as ""scruffy"", as opposed to the ""neat"" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be ""logical"", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures ""frames"". Schank used a version of frames he called ""scripts"" to successfully answer questions about short stories in English.


== Boom (1980–1987) ==
In the 1980s a form of AI program called ""expert systems"" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of connectionism in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.


=== Rise of expert systems ===
An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.In 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.


=== Knowledge revolution ===
The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. ""AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,"" writes Pamela McCorduck. ""[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay"". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.The 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.Chess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.


=== Money returns: Fifth Generation project ===
In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or ""MCC"") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.


=== Revival of neural networks ===
In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a ""Hopfield net"") could learn and process information in a completely new way. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called ""backpropagation"", also known as the reverse mode of automatic differentiation published by Seppo Linnainmaa (1970) and applied to neural networks by Paul Werbos. These two discoveries helped to revive the exploration of artificial neural networks.Starting with the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland, neural networks research gained new momentum and would become commercially successful in the 1990s, applied to optical character recognition and speech recognition.The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural network technology in the 1980s. 
A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.


== Bust: second AI winter (1987–1993) ==
The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception was that the technology was not viable. However, the field continued to make advances despite the criticism. Numerous researchers, including robotics developers Rodney Brooks and Hans Moravec, argued for an entirely new approach to artificial intelligence.


=== AI winter ===
The term ""AI winter"" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.
The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.In the late 1980s, the Strategic Computing Initiative cut funding to AI ""deeply and brutally"". New leadership at DARPA had decided that AI was not ""the next wave"" and directed funds towards projects that seemed more likely to produce immediate results.By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like ""carry on a casual conversation"" had not been met by 2010. As with other AI projects, expectations had run much higher than what was actually possible.Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that ""The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.""


=== Nouvelle AI and embodied reason ===

In the late 1980s, several researchers advocated a completely new approach to artificial intelligence, based on robotics. They believed that, to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. They argued that these sensorimotor skills are essential to higher level skills like commonsense reasoning and that abstract reasoning was actually the least interesting or important human skill (see Moravec's paradox). They advocated building intelligence ""from the bottom up.""The approach revived ideas from cybernetics and control theory that had been unpopular since the sixties. Another precursor was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)In his 1990 paper ""Elephants Don't Play Chess,"" robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."" In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.


== AI (1993–2011) ==
The field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of ""artificial intelligence"". AI was both more cautious and more successful than it had ever been.


=== Milestones and Moore's law ===
On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second.In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of ""raw computer power"" was slowly being overcome.


=== Intelligent agents ===
A new paradigm called ""intelligent agents"" became widely accepted during the 1990s. Although earlier researchers had proposed modular ""divide and conquer"" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are ""intelligent agents"", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as ""the study of intelligent agents"". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.The paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.


=== Probabilistic reasoning and greater rigor ===
AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous ""scientific"" discipline.
Judea Pearl's influential 1988 book brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for ""computational intelligence"" paradigms like neural networks and evolutionary algorithms.


=== AI behind the scenes ===
Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems
and their solutions proved to be useful throughout the technology industry, such as
data mining,
industrial robotics,
logistics,speech recognition,
banking software,
medical diagnosis
and Google's search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: ""Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""


== Deep learning, big data and artificial general intelligence: 2011–present ==
In the first decades of the 21st century, access to large amounts of data (known as ""big data""), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper ""Big data: The next frontier for innovation, competition, and productivity"" that ""by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data"".
By 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times reported that interest in AI had reached a ""frenzy"". The applications of big data began to reach into other fields as well, such as training models in ecology and for various applications in economics. Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.


=== Deep learning ===

Deep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers. According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid. As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.
However, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.
State-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the MNIST database, and traffic sign recognition.Language processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced astounding results in competing with humans, in things like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).


=== Big Data ===

Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the Big Data Era written by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The 5V characteristics of big data (proposed by IBM): Volume, Velocity, Variety, Value, Veracity.
The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing"".


=== Large language models ===

Foundation models, which are large language models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018. 
Models such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important achievements of machine learning. 
In 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".


== See also ==
History of artificial neural networks
History of knowledge representation and reasoning
History of natural language processing
Outline of artificial intelligence
Progress in artificial intelligence
Timeline of artificial intelligence
Timeline of machine learning


== Notes ==


== References ==",0.13297872340425532
"Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general-purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.


== Internet and e-commerce ==


=== Recommendation systems ===

A recommendation system predicts the ""rating"" or ""preference"" a user would give to an item. Recommendation systems are used in a variety of areas, such as generating playlists for video and music services, product recommendations for online stores, or content recommendations for social media platforms and open web content recommendation.Companies to use such systems include Netflix, Amazon and YouTube.


=== Web feeds and posts ===
Machine learning is also used in web feeds such as for determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.


=== Targeted advertising and increasing internet engagement ===

AI is used to target web advertisements to those most likely to click or engage in them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints. Both AdSense and Facebook use AI for advertising.
Online gambling companies use AI to improve customer targeting.Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.


=== Virtual assistants ===

Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.


=== Search engines ===
Search engines that use artificial intelligence include Google Search and Bing Chat.


=== Spam filtering ===

Machine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to identify any malicious elements. Numerous models built on machine learning algorithms exhibit exceptional performance with accuracies over 90% in distinguishing between spam and legitimate emails.


=== Language translation ===

AI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator. Additionally, research and development are in progress to decode and conduct animal communication.


=== Facial recognition and image labeling ===

AI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.  Facebook's DeepFace identifies human faces in digital images.


== Games ==

Games have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have produced superhuman results in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.


== Economic and social challenges ==

AI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.


== Agriculture ==

In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.


== Cyber security ==
Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.Applications of AI in cyber security include:

Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.
Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.
Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.
Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.


== Education ==

AI tutors allow students to get one-on-one help. They can reduce anxiety and stress for students stemming from tutor labs or human tutors.AI can create a dysfunctional environment with revenge effects such as technology that hinders students' ability to stay on task. In another scenario, AI can provide early prediction of student success in a virtual learning environment (VLE) such as Moodle.In the education process, students can personalize their training with the help of artificial intelligence. And for teaching professionals, the technology provided by AI can improve the quality of the educational process and teaching skills.AI text detectors can be used to scan essays generated by artificial intelligence in order to try to establish genuine authorship. However, a study found that seven of the most used of these detectors often wrongly flagged articles written by those whose first language was not English as AI-generated, thus discriminating against so called 'non-native' English speakers.


== Finance ==

Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.
Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.The use of AI in applications such as online trading and decision making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient. The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.


=== Trading and investment ===
Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.


=== Trading Companies ===
Trading companies often employ AI trading bots to enhance their potential for profitability. While they cannot assure profitability, these bots can streamline processes, saving valuable time through automation. Furthermore, they enable the testing of ideas that might be impractical to execute manually, potentially yielding highly profitable trading concepts.


=== Underwriting ===
Online lender Upstart uses machine learning for underwriting.ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.


=== Audit ===
AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.


=== Anti-money laundering ===
AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to ""develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability"". A study about deep learning for AML identified ""key challenges for researchers"" to have ""access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced"" and suggests future research should bring-out ""explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data"".


=== History ===
In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Protrader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. ""The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.""One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.


== Government ==
AI facial recognition systems are used for mass surveillance, notably in China.In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.


=== Military ===

Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. Many researchers avoid military applications.


== Health ==


=== Healthcare ===

AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.
Other healthcare tasks thought suitable for an AI that are in development include:

Screening
Heart sound analysis
Companion robots for elder care
Medical record analysis
Treatment plan design
Medication management
Assisting blind people
Consultations
Drug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)
Clinical training
Outcome prediction for surgical procedures
HIV prognosis
Identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)
Helping link genes to their functions, otherwise analyzing genes and identification of novel biological targets
Help development of biomarkers
Help tailor therapies to individuals in personalized medicine/precision medicine


=== Workplace health and safety ===

AI-enabled chatbots decrease the need for humans to perform basic call center tasks.Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.


=== Biochemistry ===
AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).


== Chemistry and biology ==

Machine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines ""computational synthesis with AI algorithms to predict molecular properties"", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for ""drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials"". It has been used for the design of proteins with prespecified functional sites.It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.


=== Novel types of machine learning ===

There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of ""machine learning"" not commonly associated with the term.Similarly, there is research and development of biological ""wetware computers"" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.
A subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world. 


==== Digital ghosts ====


==== Biological computing in AI and as AI ====
However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with ""as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it"" and that if it was understood this doesn't mean there being ""a technological solution to imitate natural intelligence"". Technologies that integrate biology and are often AI-based include biorobotics.


== Astronomy, space activities and ufology ==

Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.


=== Future or non-human applications ===

Loeb has speculated that one type of technological equipment the project may detect could be ""AI astronauts"" and in 2021 – in an opinion piece – that AI ""will"" ""supersede natural intelligence"", while Martin Rees stated that there ""may"" be more civilizations than thought with the ""majority of them"" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as ""safety of encounters with an alien AI"", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of ""AI astronauts"" that engage in ""supervised evolution"" (see also: directed evolution, uplift, directed panspermia and space colonization).


=== Astrochemistry ===
It can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.


== Other fields of research ==


=== Archaeology, history and imaging of sites ===

Machine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.
Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. 
It can also be used for ""non-invasive and non-destructive access to internal structures of archaeological remains"". 


=== Physics ===

A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.


=== Materials science ===
AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure. AI can also accelerate the accurate prediction of various properties of the materials
.


=== Reverse engineering ===
Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.


== Law ==


=== Legal analysis ===
AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.The electronic discovery industry uses machine learning to reduce manual searching.


=== Law enforcement and legal proceedings ===
COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.: 124 


== Services ==


=== Human resources ===

Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.


=== Job search ===
AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.


=== Online and telephone customer service ===
AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.


=== Hospitality ===
In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.


== Media ==

AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.
Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.

Motion interpolation
Pixel-art scaling algorithms
Image scaling
Image restoration
Photo colorization
Film restoration and video upscaling
Photo tagging
Automated species identification (such as identifying plants, fungi and animals with an app)
Text-to-image models such as DALL-E, Midjourney and Stable Diffusion
Image to video
Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google
Text to music with AI models such as MusicLM
Text to speech such as ElevenLabs and 15.ai
Motion capture


=== Deep-fakes ===
Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.
In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.
In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames. DARPA gave 68 million dollars to work on deep-fake detection.Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.


=== Video content analysis, surveillance and manipulated media detection ===

AI algorithms have been used to detect deepfake videos.


=== Music ===

AI has been used to compose music of various genres.
David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.In 2012, AI Iamus created the first complete classical album.AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.Melomics creates computer-generated music for stress and pain relief.At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.
The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.
South Korean singer Hayeon's debut song, ""Eyes on You"" was composed using AI which was supervised by real composers, including NUVO.


=== Writing and reporting ===

Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or ""how to balance the need for a coherent story progression with user agency, which is often at odds"".While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.South Korean company Hanteo Global uses a journalism bot to write articles.Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.


=== Wikipedia ===
 Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.
Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.


=== Video games ===

In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.


=== Art ===

AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing.
It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.
AI like ""Disco Diffusion"", ""DALL·E"" (1 and 2), Stable Diffusion, Imagen, ""Dream by Wombo"", Midjourney has been used for visualizing conceptual inputs such as song lyrics, certain texts or specific imagined concepts (or imaginations) in artistic ways or artistic images in general. Some of the tools also allow users to input images and various parameters e.g. to display an object or product in various environments, some can replicate artistic styles of popular artists, and some can create elaborate artistic images from rough sketches.
Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.


==== Art analysis ====
In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art.


== Utilities ==


=== Energy system ===
Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).


=== Telecommunications ===
Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.


== Manufacturing ==


=== Sensors ===
Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.


=== Toys and games ===
In the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.
Mattel created an assortment of AI-enabled toys that ""understand"" conversations, give intelligent responses, and learn.


=== Oil and gas ===
Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.


== Transport ==


=== Automotive ===

AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.
There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).


==== Traffic management ====
AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.

Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.


=== Military ===
The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.
AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
Speech recognition allows traffic controllers to give verbal directions to drones.
Artificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.


=== NASA ===
In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.


=== Maritime ===
Neural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.


== Environmental monitoring ==

Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.For example, ""Global Plastic Watch"" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.


=== Early-warning systems ===
Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.


== Computer science ==


=== Programming assistance ===


==== AI-powered code assisting tools ====
AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.
GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages. Price for individuals: $10/mo or $100/yr, with one free month trial.
Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota. Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited ""starter"" free version.CodeiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.Ghostwriter by Replit offers code completion and chat. They have multiple pricing plans, including a free one and a ""Hacker"" plan for $7/month.
CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing. Individual plan is free, professional plan is $19/user/month.
Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby


==== Neural network design ====
AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.


==== Quantum computing ====

Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).


=== Historical contributions ===
AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:
Time sharing
Interactive interpreters
Graphical user interfaces and the computer mouse
Rapid application development environments
The linked list data structure
Automatic storage management
Symbolic programming
Functional programming
Dynamic programming
Object-oriented programming
Optical character recognition
Constraint satisfaction


== Business ==


=== Customer service ===
Business websites and social media platforms for businesses like use chatbots for customer interactions like helping in answering frequently asked questions. Chatbots offers 24/7 support and replaces humans thereby helping in cutting business costs.


=== Content extraction ===
An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.


== List of applications ==


== See also ==
Applications of artificial intelligence to legal informatics
Applications of deep learning
Applications of machine learning
Collective intelligence § Applications
List of artificial intelligence projects
Progress in artificial intelligence
Open data
Timeline of computing 2020–present
Data Sources


== Footnotes ==


== Further reading ==
Kaplan, A.M.; Haenlein, M. (2018). ""Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence"". Business Horizons. 62 (1): 15–25. doi:10.1016/j.bushor.2018.08.004. S2CID 158433736.
Kurzweil, Ray (2005). The Singularity is Near: When Humans Transcend Biology. New York: Viking. ISBN 978-0-670-03384-3.
National Research Council (1999). ""Developments in Artificial Intelligence"". Funding a Revolution: Government Support for Computing Research. National Academy Press. ISBN 978-0-309-06278-7. OCLC 246584055.
Moghaddam, M. J.; Soleymani, M. R.; Farsi, M. A. (2015). ""Sequence planning for stamping operations in progressive dies"". Journal of Intelligent Manufacturing. 26 (2): 347–357. doi:10.1007/s10845-013-0788-0. S2CID 7843287.
Felten, Ed (3 May 2016). ""Preparing for the Future of Artificial Intelligence"".",0.23603461841070023
"Friendly artificial intelligence (also friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.


== Etymology and usage ==
The term was coined by Eliezer Yudkowsky, who is best known for popularizing the idea, to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:
Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.
'Friendly' is used in this context as technical terminology, and picks out agents that are safe and useful, not necessarily ones that are ""friendly"" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.


== Risks of unfriendly AI ==

The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem, or the proto-robots of Gerbert of Aurillac and Roger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict. By 1942 these themes prompted Isaac Asimov to create the ""Three Laws of Robotics""—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.In modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  He put it this way:

Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.'
In 2008 Eliezer Yudkowsky called for the creation of ""friendly AI"" to mitigate existential risk from advanced artificial intelligence. He explains: ""The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.""Steve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic ""drives"", such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, ""without special precautions"", cause the AI to exhibit undesired behavior.Alexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.Luke Muehlhauser, writing for the Machine Intelligence Research Institute, recommends that machine ethics researchers adopt what Bruce Schneier has called the ""security mindset"": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.In 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI'; nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.


== Coherent extrapolated volition ==
Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, our coherent extrapolated volition is ""our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted"".Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a ""seed AI"" programmed to first study human nature and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer. The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of ""Friendliness"", is an answer to the meta-ethical problem of defining an objective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.


== Other approaches ==

Steve Omohundro has proposed a ""scaffolding"" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.Seth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities, and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, ""existing messages about beneficial AI are not always framed well"". Baum advocates for ""cooperative relationships, and positive framing of AI researchers"" and cautions against characterizing AI researchers as ""not want(ing) to pursue beneficial designs"".In his book Human Compatible, AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:: 173 
1. The machine's only objective is to maximize the realization of human preferences.
2. The machine is initially uncertain about what those preferences are.

3. The ultimate source of information about human preferences is human behavior.
The ""preferences"" Russell refers to ""are all-encompassing; they cover everything you might care about, arbitrarily far into the future."": 173   Similarly, ""behavior"" includes any choice between options,: 177  and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.: 201 


== Public policy ==
James Barrat, author of Our Final Invention, suggested that ""a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations."" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA, which discussed risks of biotechnology.John McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health, where ""Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards."" McGinnis feels that peer review is better ""than regulation to address technical issues that are not possible to capture through bureaucratic mandates"". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.


== Criticism ==

Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in The Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be ""cautious and prepared"" given the stakes involved, we ""don't need to be obsessing"" about the risks of superintelligence. Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had. In an article in AI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.Some philosophers claim that any truly ""rational"" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful. Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis, say that it will be impossible to ever guarantee ""friendly"" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work ""only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.


== See also ==


== References ==


== Further reading ==
Yudkowsky, E. Artificial Intelligence as a Positive and Negative Factor in Global Risk. In Global Catastrophic Risks, Oxford University Press, 2008.Discusses Artificial Intelligence from the perspective of Existential risk.  In particular, Sections 1-4 give background to the definition of Friendly AI in Section 5.  Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of non-Friendly AIs.  Sections 7-13 discuss further related issues.
Omohundro, S.  2008 The Basic AI Drives Appeared in AGI-08 - Proceedings of the First Conference on Artificial General Intelligence
Mason, C. 2008 Human-Level AI Requires Compassionate Intelligence Archived 2022-01-09 at the Wayback Machine Appears in AAAI 2008 Workshop on Meta-Reasoning:Thinking About Thinking
Froding, B. and Peterson, M 2021 Friendly AI Ethics and Information Technology volume 23, pp 207–214.


== External links ==
Ethical Issues in Advanced Artificial Intelligence by Nick Bostrom
What is Friendly AI? — A brief description of Friendly AI by the Machine Intelligence Research Institute.
Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures — A near book-length description from the MIRI
Critique of the MIRI Guidelines on Friendly AI — by Bill Hibbard
Commentary on MIRI's Guidelines on Friendly AI — by Peter Voss.
The Problem with ‘Friendly’ Artificial Intelligence — On the motives for and impossibility of FAI; by Adam Keiper and Ari N. Schulman.",0.919732441471572
"Artificial intelligence art is any visual artwork created through the use of artificial intelligence (AI) programs.Artists began to create AI art in the mid to late-20th century, when the discipline was founded. In the early 21st century, the availability of AI art tools to the general public increased, providing opportunities for use outside of academia and professional artists. Throughout its history, artificial intelligence art has raised many philosophical concerns, including those related to copyright, deception, and its impact on traditional artists, including their incomes. 


== History ==


=== Early History ===
The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.  The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created in the early 1800s.The academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956, and has experienced several waves of advancement and optimism in the decades since. Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.


=== 1950s to 2000s ===
Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting AI works created by AARON, the computer program Cohen created to generate paintings, including a 1972 exhibition at the Los Angeles County Museum of Art.In both 1991 and 1992, Karl Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.In 2009, Eric Millikin won the Pulitzer Prize along with several other awards for his artificial intelligence art that was critical of government corruption in Detroit and resulted in the city's mayor being sent to jail.


=== 2010s and Deep Learning ===
In 2014, Ian Goodfellow and colleagues at Université de Montréal developed the Generative adversarial network, a type of Deep neural network capable of learning to mimic the statistical distribution of input data such as images.In 2015, a team at Google released DeepDream, a program that uses algorithmic pareidolia to create a dream-like appearance reminiscent of a psychedelic experience.In 2018, an auction sale of artificial intelligence art was held at Christie's Auction House in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for $432,500, which was almost 45 times higher than its estimate of $7,000–$10,000. The artwork was created by ""Obvious"", a Paris-based collective.In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the ""interests and culture(s) of people of color."" Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.


=== 2020s and Generative AI ===
In 2021, using the Transformer models used in GPT-2 and GPT-3, OpenAI developed DALL-E, a text-to-image AI model capable of producing high-quality images based on natural language prompts.In 2022, DALL-E was followed by Midjourney, then by the open source Stable Diffusion, leading to a dramatic growth in the use of AI to generate visual art.
In 2022, Refik Anadol created an artificial intelligence art installation at the Museum of Modern Art in New York, based on the museum's own collection.


== Tools and processes ==


=== Imagery ===
Many mechanisms for creating AI art have been developed, including procedural ""rule-based"" generation of images using mathematical patterns, algorithms which simulate brush strokes and other painted effects, and deep learning algorithms, such as generative adversarial networks (GANs) and transformers.
One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON is the most notable example of AI art in the era of GOFAI programming because of its use of a symbolic rule-based approach to generate technical images. Cohen developed AARON with the goal of being able to code the act of drawing. In its primitive form, AARON created simple black and white drawings. Cohen would later finish the drawings by painting them. Throughout the years, he also began to develop a way for AARON to also paint. Cohen designed AARON to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.
Generative adversarial networks (GANs) were designed in 2014. This system uses a ""generator"" to create new images and a ""discriminator"" to decide which created images are considered successful. DeepDream, released by Google in 2015, uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating deliberately over-processed images. After DeepDream's release, several companies released apps that transform photos into art-like images with the style of well-known sets of paintings.
The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.Several programs use text-to-image models to generate a variety of images based on various text prompts. They include EleutherAI's VQGAN+CLIP which was released in 2021, OpenAI's DALL-E which released a series of images in January 2021,  Google Brain's Imagen and Parti which was announced in May 2022, Microsoft's NUWA-Infinity, and Stable Diffusion which was released in August 2022.  Stability.ai has a Stable Diffusion web interface called DreamStudio. Stable Diffusion is source-available software, enabling further development such as plugins for Krita, Photoshop, Blender, and GIMP, as well as the Automatic1111 web-based open source user interface. Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.There are many other AI art generation programs including simple consumer-facing mobile apps and Jupyter notebooks that require powerful GPUs to run effectively.


==== Impact and applications ====
The exhibition ""Thinking Machines: Art and Design in the Computer Age, 1959–1989"" at MoMA provided an overview of AI applications for art, architecture, and design. Exhibitions showcasing the usage of AI to produce art include the 2016 Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the DeepDream algorithm and the 2017 exhibition ""Unhuman: Art in the Age of AI"", which took place in Los Angeles and Frankfurt. In spring 2018, the Association for Computing Machinery dedicated a magazine issue to the subject of computers and art. In June 2018, ""Duet for Human and Machine"", an art piece permitting viewers to interact with an artificial intelligence, premiered at the Beall Center for Art + Technology. The Austrian Ars Electronica and Museum of Applied Arts, Vienna opened exhibitions on AI in 2019. Ars Electronica's 2019 festival ""Out of the box"" explored art's role in a sustainable societal transformation.
Examples of such augmentation may include e.g. enabling expansion of noncommercial niche genres (common examples are cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, novel imaginative childhood play, very fast prototyping, increasing art-making accessibility and artistic output per effort and/or expenses and/or time – e.g. via generating drafts, inspirations, draft-refinitions, and image-components (Inpainting).


=== Prompt engineering and sharing ===

Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like ""in the style of [name of an artist]"" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image: additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.


==== Related terminology ====
Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years. 'Synthography' is a proposed term for the practice of generating images that are similar to photographs using AI.


==== Development ====
Additional functionalities are under development and may improve various applications or enable new ones – such as ""Textual Inversion"" which refers to enabling the use of user-provided concepts (like an object or a style) learned from few images. With textual inversion, novel personalized art can be generated from the associated word(s) (the keywords that have been assigned to the learned, often abstract, concept) and model extensions/fine-tuning (see also: DreamBooth).
Generated images are sometimes used as sketches or low-cost experimentations or illustration of proof-of-concept-stage ideas – additional functionalities or improvements may also relate to post-generation manual editing (polishing or artistic usage) of prompts-based art (such as subsequent tweaking with an image editor).


== Criticism, issues and controversy ==


=== Copyright ===

Ever since the beginnings of artificial intelligence art, it has sparked several debates, which in the 2020s has often concerned whether AI art can be defined as art and the impact it has on artists.In 1985, intellectual property law professor Pamela Samuelson considered the legal questions surrounding AI art authorship as it relates to copyright: who owns the copyright when the piece of art was created by artificial intelligence? Samuelson's article, ""Allocating Ownership Rights in Computer-Generated Works,"" argued that rights should be allocated to the user of the generator program. In response to the same question, a 2019 Florida Law Review article has presented three possible choices. First, the artificial intelligence itself becomes the copyright owner. To do this, Section 101 of the Copyright Act would need to be amended to define ""author"" as a natural person or a computer. Second, following Samuelson's argument, the user, programmer, or artificial intelligence company is the copyright owner. This would be an expansion of the ""work for hire"" doctrine, under which ownership of a copyright is transferred to the ""employer."" Finally, no one becomes the copyright owner, and the work would automatically enter public domain. The argument here is that because no person ""created"" the piece of art, no one should be the copyright owner.In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. Of particular issue is the use of copyrighted art within AI training datasets: in September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that ""there are no safeguards for artists to be able to identify works in databases that are being used and opt out."" Some have claimed that images generated by these models can bear an uncanny resemblance to extant artwork, sometimes including remains of the original artist's signature. Such discussion came to a head in December, when users of the portfolio platform ArtStation staged an online protest against nonconsensual use of their artwork within datasets: this resulted in opt-out services, such as ""Have I Been Trained?,"" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright, a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.An issue with many popular AI art programs is that they generate images based on artists's work without their consent. In January 2023 three artists — Sarah Andersen, Kelly McKernan, and Karla Ortiz — filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists. The same month, Stability AI was also sued by Getty Images for using its images in the training data.In July 2023, U.S. District Judge William Orrick inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.


=== Income and employment stability ===

As generative AI image software such as Stable Diffusion and DALL-E continue to advance and proliferate, the potential problems and concerns that these systems pose on creativity and artistry has risen. During 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in illustration and design industries.  In August 2022, digital artist R. J. Palmer stated that ""I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist."" Scholars Jiang et al. support this concern of job loss in creative fields by stating, “Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently,” and adding that, “This labor displacement is evident across creative industries. For instance, according to an article on Rest of World, a Chinese gaming industry recruiter has noticed a 70% drop in illustrator jobs, in part due to the widespread use of image generators; another studio in China is reported to have laid off a third of its character design illustrators.” AI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles. For example, Polish digital artist Greg Rutkowski has stated that it's more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style. Furthermore, some training databases on which AI systems are based aren't accessible to the public, which makes it impossible to know the extent to which their training data contains copyright protected images. For example, a tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that their copyrighted images were used as training data without their consent.The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed. Works of AI-generated art, such as Théâtre d'Opéra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists. These AI-created submissions have lead organizations such as Clarkesworld, a science fiction magazine, to close their submissions and only solicit works from known artists after their submission forum was flooded with texts generated by ChatGPT. 
AI-generated images have raised the concern that they can be made to damage an artist's reputation. Artist Sarah Hendersen had her art copied and then used to depict Neo-Nazi ideology. She stated that the spread of hate speech online can be worsened by the use of image generators. Jiang et al. also add to this sentiment by stating that ""tools trained on artists' works and which allow users to mimic their style without their consent or compensation, can cause significant reputational damage [by] spreading messages that they do not endorse.""


=== Deception ===
The 2023 winner of the ""creative open"" category in the Sony World Photography Awards, Boris Eldagsen, revealed after winning that his entry was actually generated by artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had ""clearly shown that even experienced photographers and art experts can be fooled"". Smaller contests have been affected as well; in 2023 a contest called the ""Self-Published Fantasy Blog-Off cover contest"", run by author Mark Lawrence, was cancelled after the winning entry was allegedly exposed to be a collage of images generated by Midjourney.Wider issues extend beyond the art world. As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading, known as ""deepfakes"".In May 2023, widespread attention was given to a Midjourney-generated photo of Pope Francis wearing a white puffer coat and another showing the fictional arrest of Donald Trump, and an AI-generated image of an attack on the Pentagon went viral as a hoax news story on Twitter.


== Analysis of existing art using AI ==
In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. This has been made possible due to large-scale digitization of artwork in the past few decades. Although the main goal of digitization was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Whereas distant viewing includes the analysis of large collections, close reading involves one piece of artwork.Researchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide fresh perspectives on the development of artistic styles and the identification of artistic influences. AI-assisted study of existing art can also aid in the organization of art exhibitions and support the decision-making process for curators and art historians.AI programs can automatically generate new images of artwork similar to those learned from the sample. Humans mainly just need to input data and discriminate output, the combination of AI mechanisms and human art creation mechanisms allows AI to produce works.


== Other generative AI ==
Some prototype robots can create what is considered forms of art – such as dynamic cooking robots that can taste and readjust.There also is AI-assisted writing beyond copy-editing (including support in the generation of fictional stories such as helping with writer's block or inspiration or rewriting segments).AI could be and has been used in video game art beyond imagery only, especially for level design (e.g. for custom maps) and creating new content or interactive stories in video games.


== See also ==
Algorithmic art
Applications of artificial intelligence#Art
Computational creativity
Cybernetic art
Generative art
List of artificial intelligence artists
Music and artificial intelligence
Neural style transfer
Synthetic media


== References ==",0.8407286314806165
"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics.


== Approaches ==


=== Machine ethics ===

Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.Isaac Asimov considered the issue in the 1950s in his I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems.  Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.
Vernor Vinge has suggested that a moment may come when some computers are smarter than humans. He calls this ""the Singularity"".  He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism. The Machine Intelligence Research Institute has suggested a need to build ""Friendly AI"", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.However, there is one technology in particular that has the potential to make the idea of morally capable robots a reality. In a paper on the acquisition of moral values by robots, Nayef Al-Rodhan mentions the case of neuromorphic chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.
In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis), while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal ""hackers"".According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller. Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.


=== Robot ethics ===

The term ""robot ethics"" (sometimes ""roboethics"") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.


=== Ethics principles of artificial intelligence ===
In the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.


==== Transparency, accountability, and open source ====
Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Ben Goertzel and David Hart created OpenCog as an open source framework for AI development. OpenAI is a non-profit AI research company created by Elon Musk, Sam Altman and others to develop open-source AI beneficial to humanity. There are numerous other open-source AI developments.
Unfortunately, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021. The IEEE effort identifies multiple scales of transparency for different stakeholders. Further, there is concern that releasing the full capacity of contemporary AI to some organizations may be a public bad, that is, do more damage than good. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted an extraordinary blog on this topic, asking for government regulation to help determine the right thing to do.Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its ""Policy and investment recommendations for trustworthy Artificial Intelligence"". This is the AI HLEG's second deliverable, after the April 2019 publication of the ""Ethics Guidelines for Trustworthy AI"". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that ""HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved"" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.


== Ethical challenges ==


=== Biases in AI systems ===

AI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's. Furthermore, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates.Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon's AI-powered recruitment tool was trained with its own recruitment data accumulated over the years, during which time the candidates that successfully got the job were mostly white males. Consequently, the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turn out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words.Large companies such as IBM, Google, etc. have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it. There are some open-sourced tools by civil societies that are looking to bring more awareness to biased AI.


=== Robot rights ===
""Robot rights"" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. These could include the right to life and liberty, freedom of thought and expression, and equality before the law. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.Experts disagree on how soon specific and detailed laws on the subject will be necessary. Glenn McGee reported that sufficiently humanoid robots might appear by 2020, while Ray Kurzweil sets the date at 2029. Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist.The rules for the 2003 Loebner Prize competition envisioned the possibility of robots having rights of their own:

61. If in any given year, a publicly available open-source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right. 
In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.The philosophy of Sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.
Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.


=== Artificial suffering ===
In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may became conscious, such as Integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an ""explosion of artificial suffering"", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances. Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be ""slightly conscious"". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.


=== Threat to human dignity ===

Joseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:

A customer service representative (AI technology is already used today for telephone-based interactive voice response systems)
A nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)
A soldier
A judge
A police officer
A therapist (as was proposed by Kenneth Colby in the 70s)Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an ""atrophy of the human spirit that comes from thinking of ourselves as computers.""Pamela McCorduck counters that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; Using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and engrained, which makes them even more difficult to spot and fight against.Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. ""When moralizing is both vehement and vague, it invites authoritarian abuse,"" he writes. Bill Hibbard writes that ""Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.""


=== Liability for self-driving cars ===

As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. Recently, there has been debate as to the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.


=== Weaponization of artificial intelligence ===

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomy. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. ""The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots."" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a ""Future of Life"" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.""If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like ""dumb robots going rogue or a network that develops a mind of its own."" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence ""escapes the constraints of biology"". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios ""seem potentially as important as the risks related to loss of control"", but research investigating AI's long-run social impact have spent relatively little time on this concern: ""this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them"".


=== Opaque algorithms ===
Approaches like machine learning with neural networks can result in computers making decisions that they and the humans who programmed them cannot explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.


=== Negligent or Deliberate Misuse of AI ===
A special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency. This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.


== Singularity ==

Many researchers have argued that, by way of an ""intelligence explosion"", a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper ""Ethical Issues in Advanced Artificial Intelligence"" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that general superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.However, instead of overwhelming the human race and leading to our destruction, Bostrom has also asserted that superintelligence can help us solve many difficult problems such as disease, poverty, and environmental destruction, and could help us to ""enhance"" ourselves.The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not ""common sense"". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.


== Institutions in AI policy & ethics ==
There are many organisations concerned with AI ethics and policy, public and governmental as well as corporate and societal.
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization.
Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.
AI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.


=== Intergovernmental initiatives ===
The European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its ""Ethics Guidelines for Trustworthy Artificial Intelligence"". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.
The OECD established an OECD AI Policy Observatory.
In 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.


=== Governmental initiatives ===
In the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the ""American AI Initiative"" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).
In January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on ""Guidance for Regulation of Artificial Intelligence Applications"" (""OMB AI Memorandum""). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.
The Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US
The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.
The Non-Human Party is running for election in New South Wales, with policies around granting rights to robots, animals and generally, non-human entities whose intelligence has been overlooked.
In Russia, the first-ever Russian ""Codex of ethics of artificial intelligence"" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.


=== Academic initiatives ===
There are three research institutes at the University of Oxford that are centrally focused on AI ethics. The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs.
The Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.
The AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.
The Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.
The Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.
Barbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.


=== NGO initiatives ===
An international non-profit organization Future of Life Institute held a 5 day conference in Asilomar in 2017 on the subject of ""Beneficial AI"", the outcome of which was a set of 23 guiding principles for the future of AI research. Through a shared vision between experts and thought leaders from variety of disciplines, this conference laid an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.


=== Private organizations ===
Algorithmic Justice League
Black in AI
Data for Black Lives
Queer in AI


== Role and impact of fiction ==

The role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.


=== History ===
Historically speaking, the investigation of moral and ethical implications of ""thinking machines"" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.


=== Impact on technological development ===
While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper, which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.
One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story  ""Runaround"", part of the short story collection  I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels – in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.
Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it?  (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)
The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.


=== TV series ===
While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.


=== Future visions in fiction and games ===
The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story ""The Planck Dive"" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games. It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.
Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.Over time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.
Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.


== Researchers ==


== Organisations ==


== See also ==


== Notes ==


== External links ==
Ethics of Artificial Intelligence at the Internet Encyclopedia of Philosophy
Ethics of Artificial Intelligence and Robotics at the Stanford Encyclopedia of Philosophy
Russell, S.; Hauert, S.; Altman, R.; Veloso, M. (May 2015). ""Robotics: Ethics of artificial intelligence"". Nature. 521 (7553): 415–418. Bibcode:2015Natur.521..415.. doi:10.1038/521415a. PMID 26017428. S2CID 4452826.
BBC News: Games to take on a life of their own
Who's Afraid of Robots? Archived 2018-03-22 at the Wayback Machine, an article on humanity's fear of artificial intelligence.
A short history of computer ethics
AI Ethics Guidelines Global Inventory by Algorithmwatch
Hagendorff, Thilo (March 2020). ""The Ethics of AI Ethics: An Evaluation of Guidelines"". Minds and Machines. 30 (1): 99–120. doi:10.1007/s11023-020-09517-8. S2CID 72940833.",-0.11715089034676664
